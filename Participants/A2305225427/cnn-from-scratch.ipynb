import random
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from PIL import Image
from tqdm import tqdm

BATCH_SIZE = 64
IMAGE_RESOLUTION = 128
NUM_EPOCHS = 10
MAX_IMAGES_PER_CLASS = 200

class ImageClassificationDataset(Dataset):
    def __init__(self, data_root, sample_limit, transform):
        self.data_root = Path(data_root)
        self.transform = transform
        self.image_samples = []
        self.label_map = {}
        label_index = 0

        for class_dir in self.data_root.iterdir():
            if not class_dir.is_dir():
                continue

            class_name = class_dir.name
            self.label_map[class_name] = label_index
            label_index += 1

            images = list(class_dir.glob("*.png"))[:sample_limit]
            images += list(class_dir.glob("*.jpg"))[:sample_limit]

            for img in images:
                self.image_samples.append((str(img), class_name))

        random.shuffle(self.image_samples)

    def __len__(self):
        return len(self.image_samples)

    def __getitem__(self, index):
        image_path, class_name = self.image_samples[index]
        try:
            image = Image.open(image_path).convert("RGB")
        except:
            return self.__getitem__(0)

        if self.transform:
            image = self.transform(image)

        return image, self.label_map[class_name]

def create_data_loaders(dataset_path):
    transform_pipeline = transforms.Compose([
        transforms.Resize((IMAGE_RESOLUTION, IMAGE_RESOLUTION)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])

    dataset = ImageClassificationDataset(
        dataset_path,
        MAX_IMAGES_PER_CLASS,
        transform_pipeline
    )

    train_count = int(0.85 * len(dataset))
    validation_count = len(dataset) - train_count

    train_set, validation_set = random_split(
        dataset,
        [train_count, validation_count]
    )

    train_loader = DataLoader(
        train_set,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0
    )

    validation_loader = DataLoader(
        validation_set,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0
    )

    return train_loader, validation_loader, len(dataset.label_map)

class SimpleConvNet(nn.Module):
    def __init__(self, class_count):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.AdaptiveAvgPool2d((4, 4))
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 16, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, class_count)
        )

    def forward(self, inputs):
        features = self.feature_extractor(inputs)
        return self.classifier(features)

def run_training():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train_loader, validation_loader, total_classes = create_data_loaders(
        "/kaggle/input/spacenet-an-optimally-distributed-astronomy-data/SpaceNet.FLARE.imam_alam/"
    )

    model = SimpleConvNet(total_classes).to(device)
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    best_validation_accuracy = 0.0

    for epoch in range(NUM_EPOCHS):
        model.train()
        correct_predictions = 0

        for images, labels in tqdm(train_loader):
            images = images.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()

            correct_predictions += (outputs.argmax(dim=1) == labels).sum().item()

        training_accuracy = 100.0 * correct_predictions / len(train_loader.dataset)

        model.eval()
        validation_correct = 0

        with torch.no_grad():
            for images, labels in validation_loader:
                images = images.to(device)
                labels = labels.to(device)
                outputs = model(images)
                validation_correct += (outputs.argmax(dim=1) == labels).sum().item()

        validation_accuracy = 100.0 * validation_correct / len(validation_loader.dataset)

        print(
            f"Epoch {epoch + 1}/{NUM_EPOCHS} "
            f"Train Accuracy: {training_accuracy:.2f}% "
            f"Validation Accuracy: {validation_accuracy:.2f}%"
        )

        if validation_accuracy > best_validation_accuracy:
            best_validation_accuracy = validation_accuracy
            torch.save(model.state_dict(), "celestial_fast_cnn.pth")

    print(f"Best Validation Accuracy: {best_validation_accuracy:.2f}%")

if __name__ == "__main__":
    run_training()
